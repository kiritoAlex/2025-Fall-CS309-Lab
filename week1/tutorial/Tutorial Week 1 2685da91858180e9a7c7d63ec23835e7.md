# Tutorial Week 1

**先修要求：** Python编程基础，HTTP基础知识

**学习目标：**

- 理解LLM的基本概念及工作原理
- 了解什么是Agent，和LLM有什么区别
- 掌握OpenAI API的核心参数与调用方法

---

## Part 1: 大语言模型基础

### 1.1 什么是LLM

**LLM (Large Language Model, 大语言模型)** 是一种基于深度学习、尤其是 **Transformer 架构** 的人工智能模型。它通过在海量文本数据上进行训练，学习语言的 **统计规律、语义关系和上下文逻辑**，从而具备生成和理解自然语言的能力。

![image.png](Tutorial%20Week%201%202685da91858180e9a7c7d63ec23835e7/image.png)

### 1.2 什么是Prompt

**Prompt（提示词）**是用户输入给大语言模型（LLM）的指令或上下文，用来引导模型生成符合预期的输出。

简单理解：**Prompt 就是和模型对话的“问题+背景+要求”**。

```jsx
messages=[
          {"role": "system", "content": "You are a helpful assistant."},
          {"role": "user", "content": "Help me write a speech"}
         ]
```

通过精心设计 Prompt，可以极大提升模型在特定任务中的表现。

### 1.3 主流大模型概览

当前主流的LLM服务提供商包括：

| 提供商 | 代表模型 | 特点 | API定价（输入/输出） |
| --- | --- | --- | --- |
| OpenAI | GPT-5 | 全面的功能与强大的推理能力 | $2.50/$10.00 per 1M tokens |
| Anthropic | Claude 4 Sonnet | 强大的代码与Agent能力，不可思议的价格 | $3.00/$15.00 per 1M tokens |
| Google | Gemini 2.5 Pro | 优秀的多模态能力与性价比 | $2.50/$10.00 per 1M tokens |

### 1.4 Token概念与计数

Token是LLM处理文本的基本单位。理解token对于成本控制和性能优化至关重要。

[https://huggingface.co/spaces/Xenova/the-tokenizer-playground](https://huggingface.co/spaces/Xenova/the-tokenizer-playground)

```python
# 实验：Token计数器
import tiktoken

# 初始化OpenAI的tokenizer
encoding = tiktoken.encoding_for_model("gpt-4")

# 测试不同文本的token数量
texts = [
    "Hello, world!",
    "你好，世界！",
    "The quick brown fox jumps over the lazy dog.",
    "人工智能正在改变我们的生活方式。"
]

for text in texts:
    tokens = encoding.encode(text)
    print(f"文本: {text}")
    print(f"Token数量: {len(tokens)}")
    print(f"Token序列: {tokens[:10]}...")  # 显示前10个token
    print("-" * 50)

```

**关键观察：**

- 英文单词通常对应1-2个token
- 中文字符通常每个字对应1-2个token
- 特殊字符和标点符号也会占用token

### 1.5 请求-响应模式

LLM API采用标准的HTTP请求-响应模式：

```python
# 基本的API调用流程
import requests
import json

def call_llm(prompt, api_key):
    """演示LLM API的基本调用结构"""
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    data = {
        "model": "gpt-4o-mini",
        "messages": [
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7
    }

    response = requests.post(
        "https://api.openai.com/v1/chat/completions",
        headers=headers,
        json=data
    )

    return response.json()
```

### 1.6 什么是Agent

在人工智能领域，**Agent（智能体）**通常指能**感知环境、做出决策并执行行动**的系统。

结合大语言模型（LLM），**Agent 就是让 LLM 不仅能“说”，还具备“做”的能力**。

![image.png](Tutorial%20Week%201%202685da91858180e9a7c7d63ec23835e7/image%201.png)

简单来说，Agent is an augmented LLM

![image.png](Tutorial%20Week%201%202685da91858180e9a7c7d63ec23835e7/image%202.png)

---

## Part 2: 快速上手实践

> 我们将使用 OpenAI 的官方 Python 库，但我们将通过一个兼容 OpenAI API 格式的第三方代理服务来访问 LLM。因此，`base_url` 被设置为了一个非官方地址，并且我们也可以通过它调用像 Gemini 这样的非 OpenAI 模型。
> 

### 2.1 环境准备

```bash
# 创建虚拟环境
python -m venv llm-lab
source llm-lab/bin/activate  # Windows: llm-lab\Scripts\activate

# 安装必要的包
pip install openai tiktoken python-dotenv requests
```

```yaml
# .env 文件
OPENAI_API_KEY=sk-*******
BASE_URL=https://api5.xhub.chat/v1
```

> 最佳实践：不要硬编码任何API_KEY，也不要让其出现在任何日志中（所以不要用`export`或者`set`命令设置环境变量，而是使用`.env`文件保存）
> 

```bash
# For Linux and MacOS
export $(cat .env | xargs) # 加载环境变量
curl ${BASE_URL}/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gemini-2.5-flash",
    "messages": [
      {
        "role": "user",
        "content": "Introduce SUSTech"
      }
    ],
    "temperature": 1.0,
    "top_p": 1.0,
    "stream": true
  }'
```

```powershell
# For windows
$jsonData = @{
    "model"       = "gemini-2.5-flash"
    "messages"    = @(
        @{
            "role"    = "user"
            "content" = "Introduce SUSTech"
        }
    )
    "temperature" = 1.0
    "top_p"       = 1.0
    "stream"      = $true
}

$headers = @{
    "Content-Type"  = "application/json"
    "Authorization" = "Bearer $env:OPENAI_API_KEY"
}

Invoke-RestMethod -Uri "$env:BASE_URL/chat/completions" -Method POST -Headers $headers -Body ($jsonData | ConvertTo-Json)

```

### 2.2 第一个LLM应用（OpenAI API）

创建 `first_llm.py`：

```python
from openai import OpenAI
import os
from dotenv import load_dotenv

# 加载环境变量
load_dotenv()

# 初始化客户端
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"), base_url = os.getenv("BASE_URL"))

def chat_with_llm(user_input):
    """与LLM进行单轮对话"""
    try:
        response = client.chat.completions.create(
            model="gemini-2.5-flash",
            messages=[
                {"role": "system", "content": "你是一个有帮助的助手。"},
                {"role": "user", "content": user_input}
            ],
            temperature=0.7,
            max_tokens=150
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"错误: {str(e)}"

# 测试
if __name__ == "__main__":
    print("LLM助手已启动！输入'退出'结束对话。")
    while True:
        user_input = input("\n你: ")
        if user_input.lower() == '退出':
            break
        response = chat_with_llm(user_input)
        print(f"助手: {response}")

```

### 2.3 OpenAI API 核心参数解析

```python
def demonstrate_parameters():
    """演示不同参数对输出的影响"""

    prompt = "写一句关于人工智能的话"

    # 1. Temperature参数（创造性）
    for temp in [0.0, 0.5, 1.0]:
        response = client.chat.completions.create(
            model="gemini-2.5-flash",
            messages=[{"role": "user", "content": prompt}],
            temperature=temp,
            n=3  # 生成3个回复
        )
        print(f"\nTemperature={temp}:")
        for choice in response.choices:
            print(f"  - {choice.message.content}")

    # 2. Max_tokens参数（长度控制）
    response = client.chat.completions.create(
        model="gemini-2.5-flash",
        messages=[{"role": "user", "content": "解释机器学习"}],
        max_tokens=50  # 限制输出长度
    )
    print(f"\n限制50 tokens的输出: {response.choices[0].message.content}")

    # 3. System消息（角色设定）
    roles = [
        "你是一位严谨的科学家",
        "你是一位5岁的小朋友",
        "你是莎士比亚"
    ]

    for role in roles:
        response = client.chat.completions.create(
            model="gemini-2.5-flash",
            messages=[
                {"role": "system", "content": role},
                {"role": "user", "content": "什么是太阳？"}
            ]
        )
        print(f"\n角色: {role}")
        print(f"回答: {response.choices[0].message.content[:100]}...")

```

### 2.4 流式传输

流式传输允许实时接收响应，提升用户体验：

```python
def stream_response(prompt):
    """演示流式响应"""
    stream = client.chat.completions.create(
        model="gemini-2.5-flash",
        messages=[{"role": "user", "content": prompt}],
        stream=True
    )

    print("助手: ", end="")
    collected_messages = []
    for chunk in stream:
        if chunk.choices[0].delta.content:
            content = chunk.choices[0].delta.content
            collected_messages.append(content)
            print(content, end="", flush=True)

    print()  # 换行
    return "".join(collected_messages)

# 测试流式输出
stream_response("用100字介绍量子计算")
```

---

## 扩展阅读

1. **OpenAI官方文档**: [platform.openai.com/docs](https://platform.openai.com/docs)
2. **Anthropic Agent指南**: Building Effective AI Agents

---

**思考题**：

- 如何设计一个能记住聊天信息的LLM对话程序？
- 参数中的`top_p`/`top_k`分别是什么意思？
- 如何通过设计Prompt来让LLM学会如何调用工具？