# Tutorial Week 3

思考以下场景：

- 你是一家企业的老板，你希望能利用LLM的能力制造出一个智能客服，它能回答关于你们公司的业务问题
- 你是一个程序员，你希望大模型能查阅API文档给出精准回答，而不是凭空捏造不存在的API
- 你是一个学生，你希望在向大模型提问的时候，模型不仅可以给出回答，还可以对应到教科书上的具体知识点

但是现有前沿LLM都是预训练模型，掌握的知识仅限于训练语料，无法解决上述实际需求

是否存在一种合适的解决方案？

向模型提问时，将需要的文档一起发送给模型？（上下文？效率？成本？推理速度？）

# RAG（Retrieve Augmented Generation）

先从知识库中检索内容，再基于这些内容生成答案

## 基本流程

![image.png](Tutorial%20Week%203%202765da91858180d791b8f5cb81da951c/image.png)

1. 用户将文档上传到知识库后，系统将文档分片，分割成一系列小文档
2. 用户提问后，系统从知识库中找出最符合答案的几个小文档
3. 系统将这些小文档作为上下文嵌入提示词，然后发送给模型
4. 模型根据这些信息生成更优质的回答

RAG的流程大致可以分为以下五个部分：分片、索引、召回、重排、生成

## 分片

分片的目的是将文档细化为更小的部分，从而方便后续发送给模型

常见的分片方式主要有以下几种：

- 按字数划分
- 按段落划分
- 按章节划分
- 按页码划分
- 前沿划分技术

![image.png](Tutorial%20Week%203%202765da91858180d791b8f5cb81da951c/image%201.png)

## 索引

通过Embedding将片段文本转化为向量，然后将片段原始文本和片段向量存入向量数据库

![image.png](Tutorial%20Week%203%202765da91858180d791b8f5cb81da951c/image%202.png)

**向量及其应用**

向量可以用于存储信息，语义接近的两个词或两句话在向量空间中的方向也很相近

我们将两个片段通过embedding转化为向量后，即可通过它们向量之间的相似度来判断两个片段在语义上的关联性

![image.png](Tutorial%20Week%203%202765da91858180d791b8f5cb81da951c/image%203.png)

![image.png](Tutorial%20Week%203%202765da91858180d791b8f5cb81da951c/image%204.png)

将片段Embedding为向量使用的是模型，不同Embedding模型的排行可以参考以下链接

[https://huggingface.co/spaces/mteb/leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

![image.png](Tutorial%20Week%203%202765da91858180d791b8f5cb81da951c/image%205.png)

**向量数据库**

用于存储和查询向量的数据库，为存储向量做了很多优化，还支持计算向量相似度的函数

一般向量数据库表格至少有原始文本和对应向量两列

## 召回

搜索与用户问题相关的片段

- 用户提出一个问题，Embedding模型将其嵌入为向量
- 将嵌入向量发送给向量数据库，查询最相似的的十个向量
- 将最相似的十个向量对应的原始文本返回

```jsx
Similirity（[11,5,2,3,1]， [4.8,3.7,1.5,5.2,6.0]）= 0.7506
```

**向量相似度计算方法**

余弦相似度：两个向量方向夹角越小，相似度越高

欧氏距离：计算两个向量之间的距离，距离越小相似度越高

点积：考虑两个向量的方向关系和长度关系，点积越大相似度越高

## 重排

重排就是重新排序，本质上也是一种召回

我们要把召回得到的十份原始文本再度细化，从中提取出三份最相关的文本出来

为什么不直接在召回阶段提取三份最相关的文本呢？

**重排阶段使用的文本相似度计算逻辑不同（cross-encoder）**

把 query 和 document 拼接在一起，输入到同一个 Transformer 模型中，最后直接输出一个相关性分数。计算时需要对 query–document ****每一对组合都跑一遍模型，开销大但精度高

```jsx
Query: "Who wrote The Old Man and the Sea?"
Document: "The Old Man and the Sea was written by Ernest Hemingway in 1951."

[CLS] Who wrote The Old Man and the Sea? [SEP] The Old Man and the Sea was written by Ernest Hemingway in 1951. [SEP]
```

Transformer 处理后，模型会直接输出一个 **相关性分数**（如 0.95，表示很相关）。

| 维度 | 召回（Retrieval） | 重排（Re-ranking） |
| --- | --- | --- |
| **目标** | 从大库中找候选文档 | 对候选文档做精细排序 |
| **数量** | 通常几十到几百 | 通常前 5–10 |
| **优先级** | 快速覆盖，保证不漏掉 | 精细筛选，保证相关性 |
| **模型复杂度** | 轻量（embedding 相似度，BM25） | 重（cross-encoder，LLM） |
| **性能要求** | 高速，低延迟 | 允许更慢，因为文档量少 |
| **类比** | 大海捞针，先捞一堆针 | 从这堆针里挑最尖的几根 |

## 生成

获得最贴合用户问题的文档片段后，将其嵌入到系统提示词中，从而达到增强生成质量的目的

```jsx
`You are a helpful AI assistant for an educational course. Use the following context from course materials to answer the user's question.

Course Materials Context:
${Context}
Conversation History:
${historyContext}

Guidelines:
- Answer based only on the provided course materials
- Be concise and educational
- If the context doesn't contain the answer, say you don't have enough information
- Cite specific parts of the materials when possible
- Be helpful and encouraging

User Question: ${question}

Provide a clear, educational answer:`
```